---
title: "On AI and Authorship"
description: "On using AI to build this site, and what that means for authorship."
---

This site was mostly made by a machine. I should probably tell you about that.

I used generative AI — mostly large language models — to write the prose, draft the code, design the experiments, and build the theme you're looking at. I chose the topics. I steered the direction. I decided what was good enough and what needed another pass. But the vast majority of the words and the code came out of a model, not out of my fingers.

So whose work is this?

## The "We" Thing

You'll notice I say "we" a lot in these posts. "We built a neural network." "We'll explore this next." It's a convention borrowed from textbooks and technical writing — the pedagogical "we," an invitation to the reader to come along for the ride.

But it's also a dodge. Saying "I" felt wrong, because I didn't write most of this myself. Saying "the AI and I" every few sentences would be insufferable. "We" papers over the ambiguity in a way that's convenient, and I'll admit, a little pretentious. I'm keeping it — but I wanted to name it.

## The Sculpture

Here's a thought experiment I keep coming back to.

An artist sees a poster for a movie. The movie was based on a book, though the screenwriter changed the story substantially. The poster's composition sparks an idea. The artist sketches a design for a sculpture, then hires an artisan to fabricate the metal components. Volunteers assemble the final piece, each using their own judgment about where their section goes.

Whose art is it? Who was creative?

The artist, obviously — they had the vision. But the artisan's craftsmanship shapes the final form. The volunteers' choices give it texture the artist didn't plan. The screenwriter's divergence from the source material is what made the poster interesting in the first place. And the author of the book started it all, though they might not recognize what it became.

Creativity has always been like this. It's remixing, all the way down. The question isn't whether something is original — nothing is — but whether the chain of transformation produced something that has its own meaning, its own purpose. Whether someone took responsibility for the whole.

When I use an AI to write a blog post, I'm somewhere in that chain. I'm not the artisan. I'm not even the artist in the traditional sense. I'm more like the person who had the idea, hired the talent, and decided when it was done. Is that enough?

I genuinely don't know.

## What It Means to Know Something

There's a related question that nags at me. These posts cover neural networks, data structures, distributed systems. I've studied these topics. I have opinions about them. But did I develop a deeper understanding by having an AI explain them back to me in prose I then edited? Or did I just technically-accurately skim?

When I was in school, writing was how I learned. The act of struggling to explain something — choosing words, finding the right analogy, restructuring an argument that wasn't working — was the thinking itself. If the AI does the struggling, what am I left with? Editorial judgment, maybe. A sense of quality. But not the same depth.

I think the honest answer is: it depends on the post. Some of these I could teach from memory. Others I'd need to re-read my own site to talk about. That's a strange position to be in as the "author."

## The Serious Part

None of this should distract from a harder question: the models I used were trained on other people's work, often without their consent or compensation. This isn't a hypothetical concern. Writers, artists, and developers have had their labor ingested into training data that now competes with them.

I don't have a clean resolution for this. I benefit from the tools. I also think the people whose work made those tools possible deserve better than what they've gotten. These aren't contradictory positions — they're the same tension that runs through every era of technological disruption, and "it's always been this way" doesn't make it just.

What I can do:

- Be transparent about what's AI-assisted (hence this post, and the disclosures on the home page and footer)
- Credit sources and inspirations explicitly
- Stay informed about how the ethics and law are evolving
- Support frameworks that compensate creators fairly

<!-- TODO: Research and expand this section. Look into:
  - The NYT v. OpenAI lawsuit and its implications
  - The EU AI Act's transparency requirements
  - Creative Commons and opt-out frameworks
  - How other bloggers/creators are handling disclosure
  - The "fair use" debate in the context of training data
-->

## Where This Leaves Me

I think the answer, at least for now, is radical transparency. Not as a shield ("I told you it was AI, so it's fine") but as an ongoing practice. I want this site to reflect my actual interests, my actual judgment, my actual sense of what's worth exploring. The AI is a tool — an extraordinarily powerful one — and I'm responsible for what I point it at and what I publish under my name.

If that makes the whole project feel less impressive, that's probably correct. And if it makes you think differently about what you're reading, good. You should.
